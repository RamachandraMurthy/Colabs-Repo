{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTPN32Yt8v3eNd4OiI3V48",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RamachandraMurthy/Colabs-Repo/blob/master/Langchain_Directory_loader_V2_0_Certified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Program by Ramachandra Murthy\n",
        "\n",
        "03/28/2023\n",
        "\n",
        "This is a perfect library which loads the files from a given directory. It goes in to recursive in to sub folders as well. This can be used as a initial program."
      ],
      "metadata": {
        "id": "TcNu0aD2CJPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a Directory Loader from the Langchain github repository. This is alternate to my previous version."
      ],
      "metadata": {
        "id": "zDKZQpbNhiqD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i06az7nphhFa"
      },
      "outputs": [],
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q unstructured\n",
        "!pip install -q unstructured[local-inference]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define CSVLoader class\n",
        "from csv import DictReader\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders.base import BaseLoader\n",
        "\n",
        "class CSVLoader(BaseLoader):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        file_path: str,\n",
        "        source_column: Optional[str] = None,\n",
        "        csv_args: Optional[Dict] = None,\n",
        "        encoding: Optional[str] = None,\n",
        "    ):\n",
        "        self.file_path = file_path\n",
        "        self.source_column = source_column\n",
        "        self.encoding = encoding\n",
        "        if csv_args is None:\n",
        "            self.csv_args = {\n",
        "                \"delimiter\": \",\",\n",
        "                \"quotechar\": '\"',\n",
        "            }\n",
        "        else:\n",
        "            self.csv_args = csv_args\n",
        "\n",
        "    def load(self) -> List[Document]:\n",
        "        docs = []\n",
        "\n",
        "        with open(self.file_path, newline=\"\", encoding=self.encoding) as csvfile:\n",
        "            csv = DictReader(csvfile, **self.csv_args)\n",
        "            for i, row in enumerate(csv):\n",
        "                content = \"\\n\".join(f\"{k.strip()}: {v.strip()}\" for k, v in row.items())\n",
        "                if self.source_column is not None:\n",
        "                    source = row[self.source_column]\n",
        "                else:\n",
        "                    source = self.file_path\n",
        "                metadata = {\"source\": source, \"row\": i}\n",
        "                doc = Document(page_content=content, metadata=metadata)\n",
        "                docs.append(doc)\n",
        "\n",
        "        return docs\n"
      ],
      "metadata": {
        "id": "CuYuMBSYO47k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries and define DirectoryLoader class\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import List, Type, Union\n",
        "\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.document_loaders.base import BaseLoader\n",
        "from langchain.document_loaders.html_bs import BSHTMLLoader\n",
        "from langchain.document_loaders.text import TextLoader\n",
        "from langchain.document_loaders.unstructured import UnstructuredFileLoader\n",
        "\n",
        "FILE_LOADER_TYPE = Union[\n",
        "    Type[UnstructuredFileLoader], Type[TextLoader], Type[BSHTMLLoader]\n",
        "]\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def _is_visible(p: Path) -> bool:\n",
        "    parts = p.parts\n",
        "    for _p in parts:\n",
        "        if _p.startswith(\".\"):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "class DirectoryLoader(BaseLoader):\n",
        "    \"\"\"Loading logic for loading documents from a directory.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        path: str,\n",
        "        glob: str = \"**/[!.]*\",\n",
        "        silent_errors: bool = False,\n",
        "        load_hidden: bool = False,\n",
        "        loader_cls: FILE_LOADER_TYPE = UnstructuredFileLoader,\n",
        "        recursive: bool = False,\n",
        "    ):\n",
        "        self.path = path\n",
        "        self.glob = glob\n",
        "        self.load_hidden = load_hidden\n",
        "        self.loader_cls = loader_cls\n",
        "        self.silent_errors = silent_errors\n",
        "        self.recursive = recursive\n",
        "\n",
        "    def load(self) -> List[Document]:\n",
        "        p = Path(self.path)\n",
        "        docs = []\n",
        "        items = p.rglob(self.glob) if self.recursive else p.glob(self.glob)\n",
        "        for i in items:\n",
        "            if i.is_file():\n",
        "                if _is_visible(i.relative_to(p)) or self.load_hidden:\n",
        "                    try:\n",
        "                        if i.suffix == \".csv\":\n",
        "                            sub_docs = CSVLoader(str(i)).load()\n",
        "                        else:\n",
        "                            sub_docs = self.loader_cls(str(i)).load()\n",
        "                        docs.extend(sub_docs)\n",
        "                    except Exception as e:\n",
        "                        if self.silent_errors:\n",
        "                            logger.warning(e)\n",
        "                        else:\n",
        "                            raise e\n",
        "        return docs\n",
        "\n",
        "    def list_files(self) -> List[str]:\n",
        "        p = Path(self.path)\n",
        "        file_paths = []\n",
        "        items = p.rglob(self.glob) if self.recursive else p.glob(self.glob)\n",
        "        for i in items:\n",
        "            if i.is_file():\n",
        "                if _is_visible(i.relative_to(p)) or self.load_hidden:\n",
        "                    file_paths.append(str(i))\n",
        "        return file_paths\n"
      ],
      "metadata": {
        "id": "kDLANSDjOelN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "faEktf_DOrwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize DirectoryLoader\n",
        "path = \"/content/drive/MyDrive/Colab Notebooks/Data/\"\n",
        "glob_pattern = \"**/[!.]*\"\n",
        "recursive = True\n",
        "\n",
        "loader = DirectoryLoader(path, glob=glob_pattern, recursive=recursive)"
      ],
      "metadata": {
        "id": "b3ja410TOvSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List and sort file names, and print the summary\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "file_names = loader.list_files()\n",
        "sorted_file_names = sorted(file_names, key=lambda x: Path(x).suffix)\n",
        "file_type_counts = defaultdict(int)\n",
        "for file_name in sorted_file_names:\n",
        "    file_type = Path(file_name).suffix\n",
        "    file_type_counts[file_type] += 1\n",
        "\n",
        "total_files = len(sorted_file_names)\n",
        "unique_file_types = len(file_type_counts)\n",
        "\n",
        "print(f\"Total Files: {total_files}\")\n",
        "print(f\"Unique File Types: {unique_file_types}\")\n",
        "print(\"File Type Counts:\")\n",
        "for file_type, count in file_type_counts.items():\n",
        "    print(f\"{file_type}: {count}\")\n",
        "\n",
        "print(\"\\nFile Names:\")\n",
        "for file_name in sorted_file_names:\n",
        "    print(file_name)\n"
      ],
      "metadata": {
        "id": "fX5UWjXKO1O6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With the following code to load multiple files from the same directory:\n",
        "data = []\n",
        "file_data = loader.load()\n",
        "data.extend(file_data)"
      ],
      "metadata": {
        "id": "SPPapb6gkDuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk your data up into smaller documents\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(data)\n",
        "print(f'Now you have {len(texts)} documents')\n"
      ],
      "metadata": {
        "id": "LYYZefe0kNVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pinecone-client\n",
        "!pip install -q openai\n",
        "\n",
        "from langchain.vectorstores import Chroma, Pinecone\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.llms import OpenAI\n",
        "import pinecone\n",
        "import openai"
      ],
      "metadata": {
        "id": "pTX44uCZk1tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your OpenAI API key\n",
        "openai.api_key = \"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\"  # Replace with your actual OpenAI API key\n",
        "\n",
        "# Set your Pinecone API key and environment\n",
        "pinecone.api_key = \"9ded3df6-4867-42af-bcd6-56c365484778\"  # Replace with your actual Pinecone API key\n",
        "pinecone_api_env = \"us-central1-gcp\"  # Replace with your Pinecone environment\n"
      ],
      "metadata": {
        "id": "ZuKSLluYkymQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embeddings of your documents to get ready for semantic search\n",
        "# Please note the index name is from pinecone service\n",
        "# Pinecone.io (https://app.pinecone.io)\n",
        "\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=openai.api_key)\n",
        "pinecone.init(api_key=pinecone.api_key, environment=pinecone_api_env)\n",
        "index_name = \"dxcpolicy\"\n",
        "docsearch = Pinecone.from_texts([t.page_content for t in texts], embeddings, index_name=index_name)"
      ],
      "metadata": {
        "id": "78bgN3aGkeFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the QA chain with the OpenAI language model\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "llm = OpenAI(temperature=0, openai_api_key=openai.api_key)\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ],
      "metadata": {
        "id": "Boqm5MrNlQ1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------"
      ],
      "metadata": {
        "id": "14woHlbilU3k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test below before testing it thru gradio interface"
      ],
      "metadata": {
        "id": "gUT-cAvelZwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How many are listed under Ramachandra Murthy\"\n",
        "docs = docsearch.similarity_search(query, include_metadata=True)"
      ],
      "metadata": {
        "id": "cNmPbwZclW6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.run(input_documents=docs, question=query)"
      ],
      "metadata": {
        "id": "6H8zrFcIldZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------"
      ],
      "metadata": {
        "id": "tTzj7YL7lV22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio\n",
        "!pip install --upgrade pillow\n"
      ],
      "metadata": {
        "id": "ygy7zCynoPZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "xoDgXyVKphJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def semantic_search_qa(query):\n",
        "    if query:\n",
        "        # Perform a similarity search on the document embeddings\n",
        "        docs = docsearch.similarity_search(query, include_metadata=True)\n",
        "\n",
        "        # Run the QA chain with the found documents and the query\n",
        "        answer = chain.run(input_documents=docs, question=query)\n",
        "        return answer\n",
        "\n",
        "# Create Gradio interface for the semantic search and question-answering process\n",
        "def launch_gradio_interface():\n",
        "    inputs = gr.inputs.Textbox(lines=7, label=\"Ask a question\")\n",
        "    outputs = gr.outputs.Textbox(label=\"Answer\")\n",
        "\n",
        "    gr.Interface(fn=semantic_search_qa, inputs=inputs, outputs=outputs, title=\"DXC Policy Search and QA by Ramachandra Murthy\",\n",
        "                 description=\"Ask any DXC policy question\",\n",
        "                 theme=\"compact\").launch(share=True)"
      ],
      "metadata": {
        "id": "dAFK8HXPoX9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the Gradio interface function\n",
        "launch_gradio_interface()"
      ],
      "metadata": {
        "id": "EivDnQCSoZxp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}